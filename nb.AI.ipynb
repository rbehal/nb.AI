{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 99\n",
    "pd.options.display.max_columns = 99\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRawData(year):\n",
    "\n",
    "    # URL page we will scraping (see image above)\n",
    "    url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\".format(year)\n",
    "    # this is the HTML from the given URL\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # use findALL() to get the column headers\n",
    "    soup.findAll('tr', limit=2)\n",
    "    # use getText()to extract the text we need into a list\n",
    "    headers = [th.getText() for th in soup.findAll('tr', limit=2)[0].findAll('th')]\n",
    "    # exclude the first column as we will not need the ranking order from Basketball Reference for the analysis\n",
    "    headers = headers[1:]\n",
    "\n",
    "    # avoid the first header row\n",
    "    rows = soup.findAll('tr')[1:]\n",
    "    player_stats = [[td.getText() for td in rows[i].findAll('td')]\n",
    "                for i in range(len(rows))]\n",
    "    \n",
    "    stats = pd.DataFrame(player_stats, columns = headers)\n",
    "    stats = stats.drop('Pos', axis=1)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(stats):\n",
    "    # Deleting duplicates\n",
    "    stats.drop_duplicates(subset='Player', keep='first',inplace=True)\n",
    "    \n",
    "    stats.set_index(\"Player\",inplace=True) # Player is index value\n",
    "\n",
    "    # Dropping unrelated values/unknowns\n",
    "    stats.drop('Tm', 1, inplace=True)\n",
    "    stats.dropna(inplace=True)\n",
    "\n",
    "    # One hot encoding player position\n",
    "#     stats = pd.get_dummies(stats, columns=['Pos'])\n",
    "\n",
    "\n",
    "    # Checking for autocorrelation and dropping those columns\n",
    "    stats = stats.apply(pd.to_numeric)\n",
    "    corr = stats.corr()\n",
    "\n",
    "    threshold = 0.90\n",
    "\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= threshold:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    \n",
    "    selected_columns = stats.columns[columns]\n",
    "    stats = stats[selected_columns]\n",
    "\n",
    "    # Standardizing feature dataset\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(stats)\n",
    "    x_scaled = scaler.transform(stats)\n",
    "    stats_x = pd.DataFrame(x_scaled, columns = stats.columns)\n",
    "    \n",
    "#     stats_x = stats_x.iloc[:, :12]\n",
    "\n",
    "    # Replacing NAN values after standardization\n",
    "    stats_x = stats_x.fillna(0)\n",
    "\n",
    "    # Concatenating standardized data with one hot encoded data\n",
    "#     pos_stats = stats.iloc[:,11:]\n",
    "#     pos_stats.reset_index(inplace = True)\n",
    "#     pos_stats.drop('Player', 1, inplace=True)\n",
    "    \n",
    "    #print(\"POS STATS: \", pos_stats, \"STATS_X\", stats_x)\n",
    "\n",
    "#     pp_data = pd.concat([stats_x,pos_stats],axis=1) # Final preprocessed data\n",
    "    \n",
    "    return stats_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTarget():\n",
    "    mvp = []\n",
    "    for i in range(530):\n",
    "        if i == 18:\n",
    "            mvp.append(1)\n",
    "        else :\n",
    "            mvp.append(0)\n",
    "    mvp_df = pd.DataFrame(mvp)\n",
    "    mvp_df = mvp_df.rename(columns={0:\"MVP\"})\n",
    "    return mvp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_2018 = createRawData(2018)\n",
    "stats_2019 = createRawData(2019)\n",
    "pp_2018 = preprocessData(stats_2018)\n",
    "pp_2019 = preprocessData(stats_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pp_2019\n",
    "y = addTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = RandomFore\n",
    "lr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lr.predict_proba(pp_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = 0\n",
    "index = None\n",
    "arr = results[:,0]\n",
    "for i in range(0, arr.size):\n",
    "    if (arr[i] > max_):\n",
    "        max_ = arr[i]\n",
    "        index = i\n",
    "print(\"Maximum value: {} at index {}\".format(max_, index))\n",
    "results[:,1][index]\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_2018.iloc[150,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.shape)\n",
    "print(stats_2018.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#               'alpha': (1e-1,1e-2, 1e-3,1e-4,1),\n",
    "#               'max_iter': (10,100,1000),\n",
    "#               'penalty': ('l2', 'l1', 'elasticnet') \n",
    "#              }\n",
    "# gs_clf = GridSearchCV(SGDClassifier(), parameters, n_jobs=-1, verbose=10)\n",
    "# gs_clf = gs_clf.fit(X,y)\n",
    "# print(gs_clf.best_score_)\n",
    "# print(gs_clf.best_params_)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
